{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12d2ad37-f864-41e1-a61c-88e72acc1b75",
   "metadata": {},
   "source": [
    "# Analyzing the Wine Dataset\n",
    "\n",
    "#### In this analysis, you will use two machine learning methods implemented in python to predict the cultivar of wine based on its chemical features.\n",
    "\n",
    "#### Please take your time with this analysis. Run each command (using Shift + Enter) and think about what you are asking the computer to do.\n",
    "\n",
    "#### The wine dataset is the result of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines. Though initially used for fairly naive discrimination methods, it is an early example of machine learning for predictive data in biology.\n",
    "\n",
    "#### The first thing we will do is load the data, which is included as part of the sklearn python module. If you end up using machine learning in your practcal projects, it is likely that sklearn will be of use to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e320a8-bc44-48cb-9ae3-0419a2fb398e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the function to load the dataset\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# load the dataset\n",
    "wine_dataset = load_wine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6555c4-5a49-4b48-a3a4-5d85407a0281",
   "metadata": {},
   "source": [
    "#### We can quickly check a few things to ensure our data are loaded.\n",
    "\n",
    "> #### 1. The column headers (keys)\n",
    "> #### 2. The values of the first 5 columns of data\n",
    "> #### 3. The cultivar each row's data belong to (Targets)\n",
    "> #### 4. The names of those cultivars\n",
    "> #### 5. The feature (variable) names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5938448d-961d-4323-9efc-d2d24a31ea8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the keys\n",
    "print(\"Keys of wine_dataset: \\n{}\".format(wine_dataset.keys()))\n",
    "print(\"\\n\")\n",
    "\n",
    "# print the first five columns\n",
    "print(\"First five columns of data:\\n{}\".format(wine_dataset['data'][:5]))\n",
    "print(\"\\n\")\n",
    "\n",
    "# print the targets\n",
    "print(\"Targets:\\n{}\".format(wine_dataset['target'][:]))\n",
    "print(\"\\n\")\n",
    "\n",
    "# print the target names\n",
    "print(\"Target names:\\n{}\".format(wine_dataset['target_names']))\n",
    "print(\"\\n\")\n",
    "\n",
    "# print the feature names\n",
    "print(\"Feature names:\\n{}\".format(wine_dataset['feature_names']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3601092f-e442-49ae-8166-e5261a88263b",
   "metadata": {},
   "source": [
    "#### We can also look at the description of the dataset provided by one of the hardworking folks at sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775790e6-b497-499a-8f48-f92098c1b479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the description\n",
    "print(wine_dataset['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a73529e",
   "metadata": {},
   "source": [
    "#### OK - we can be fairly confident that our data are loaded correctly.\n",
    "\n",
    "#### Now we are going to filter our data set to contain only 4 variables, to make it slightly easier to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e05421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the modules we need\n",
    "import numpy as np\n",
    "\n",
    "# choose the features we want to keep\n",
    "selected_features = ['alcohol', 'malic_acid', 'total_phenols', 'flavanoids']\n",
    "\n",
    "# find their column numbers\n",
    "indices = [wine_dataset.feature_names.index(f) for f in selected_features]\n",
    "\n",
    "# filter the data\n",
    "filtered_data = wine_dataset.data[:, indices]\n",
    "\n",
    "# update the format of the new dataset\n",
    "from sklearn.utils import Bunch\n",
    "wine = Bunch(\n",
    "    data=filtered_data,\n",
    "    target=wine_dataset.target,\n",
    "    feature_names=selected_features,\n",
    "    target_names=wine_dataset.target_names,\n",
    "    DESCR=wine_dataset.DESCR,\n",
    "    frame=None\n",
    ")\n",
    "\n",
    "# check the format of our filtered dataset looks right\n",
    "print(wine.feature_names)\n",
    "print(wine.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee96216-b930-4be6-89a7-f089c3a21b96",
   "metadata": {},
   "source": [
    "#### Now we need to set our data up for analysis. We are using the handy python function \"train_test_split\" from the sklearn module. This converts our data into a test set and a training set. Have a look at how the funciton is implemented here and consider the arguments. random_state is set to 0 - don't worry about this. It's just the random seed and by fixing it we guarentee that we all get the same result. Normally, this would not be set.\n",
    "\n",
    "\n",
    "### Questions\n",
    ">#### 1. Why do we split our dataset into a training set and a test set?\n",
    ">#### 2. What proportion of the dataset will be included in the training set?\n",
    ">#### 3. Given our use of train_test_split, do you think the algorithms we will be using are supervised, or unsupervised method?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fc3cb7-96d4-47c5-bb85-fa3b10bb8dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        wine['data'], wine['target'], test_size=0.25, random_state=0)\n",
    "print(\"X_train shape: {}\".format(X_train.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))\n",
    "print(\"X_test shape: {}\".format(X_test.shape)) \n",
    "print(\"y_test shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f58765b-e69d-4c55-a966-fb5dcd01e5b9",
   "metadata": {},
   "source": [
    "#### The output of the above tells you the dimentions of the data - what it's saying is that, for example, the training dataset is 133 wines with 4 features plus the Y variable (cultivar)\n",
    "\n",
    "#### Now we will convert the data into a [\"pandas\"](https://pandas.pydata.org/docs/user_guide/index.html) dataframe. As far as I'm aware, pandas has nothing to do with the [black and white balls of clumsyness that they have in Edinburgh zoo](https://www.wwf.org.uk/learn/fascinating-facts/pandas). More boringly, it is a set of functions that allow us to interpret python data in data-frames, like you might be used to in R. A lot of sklearn functions rely on pandas data structures.\n",
    "\n",
    "#### We can then plot the 4 measurement variables against each other, observing how they correlate and how cultivars tend to cluster with each other. We call this a [scatter matrix](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.plotting.scatter_matrix.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de8ee85-b2cc-4cba-8352-83e81b9b8cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the modules we will use\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e2cffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the data to a pandas dataframe\n",
    "wine_dataframe = pd.DataFrame(X_train, columns=wine.feature_names)\n",
    "\n",
    "# plot the four measurements against each other\n",
    "grr = pd.plotting.scatter_matrix(wine_dataframe, c=y_train, figsize=(15, 15), marker='o',hist_kwds={'bins': 20}, s=60, alpha=.8)\n",
    "plt.figure()\n",
    "plt.imshow([np.unique(wine.target)])\n",
    "_ = plt.xticks(ticks=np.unique(wine.target),labels=wine.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81125ed5-97a5-4d8b-a43e-40adf68b79c2",
   "metadata": {},
   "source": [
    "### Questions\n",
    ">#### 4. Given the scatter matrix above, which two features will be most likely to create an accurate model with which to predict the cultivar of wine?\n",
    "\n",
    ">#### 5. Which two features would be the least likely to create an accurate model?\n",
    "\n",
    "#### Right. Now you get to do some actual computation. We will be implementing the [K Neighbours](https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-classification) algorithm to create a model to predict the cultivar of wine from the amount of total phenols and flavenoids, which you hopefully identified as the most informative pair of features above. K Neighbours is a very simple method whose model is built simply by asking for a datapoint, the identify of the nearest K neighbours. The majority vote amounts to the prediction for that point. There is one parameter - n_neighbors (K). This is the size of the sample of nearest neighbours to make a prediction from. i.e. K=1 - the prediction is the identity of the nearest neighbour. K=3 - the prediction is the identity of the highest number of the nearest 3 neighbours. In our case, we will set this to one. Have a think about how changing this might affect your predictions.\n",
    "\n",
    "#### Technically, unless we are estimating K, which we are not here, K nearest neighbours is not machine learning, but a lot of the principles that we learned in the lecture apply.\n",
    "\n",
    "#### The way that sklearn works is, to my mind, a bit unintuitive. First, you create a variable that features all of the hyperparameters with which you can make your model (here that variable is knn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e156fd3b-1cbf-4abb-8ca5-30d257d72b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the function we will use\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# create the model classifier variable\n",
    "knn = KNeighborsClassifier(n_neighbors=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ac8225-418b-46d1-8e46-2b59ee361750",
   "metadata": {},
   "source": [
    "#### Then you can fit the model on the data - we have here chosen to fit it to the total phenols and flavenoids `(X_train[:,2:5])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc6b050-2691-48e1-be0d-f0424648adcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model to our data\n",
    "knn.fit(X_train[:,2:5], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f1216f-0531-4af3-a5b7-e2899231117d",
   "metadata": {},
   "source": [
    "#### We can visualise our model using the following code - don't worry too much about the code. Just know that it first sets up the plot area, then colours the plot according to the mode estimated, then plots the training data and in a colour corresponding to the cultivar according to a legend.\n",
    "\n",
    "#### The code below produces a warning about colours which is to boring to concern us. Don't worry about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0a6a02-2bb4-4a36-a19d-ba3e28cd7c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the functions we will use\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot the model and the data\n",
    "n_neighbors = 1\n",
    "x_min,x_max = X_train[:,2].min() - 1, X_train[:,2].max()+ 1\n",
    "y_min,y_max = X_train[:,3].min() - 1, X_train[:,3].max()+ 1\n",
    "h=0.02\n",
    "xx,yy = np.meshgrid(np.arange(x_min,x_max,h),np.arange(y_min,y_max,h))\n",
    "Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "cmap_bold = ListedColormap(['darkorange', 'c', 'darkblue'])\n",
    "cmap_light=ListedColormap(['orange', 'cyan', 'cornflowerblue'])\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.pcolormesh(xx, yy, Z, cmap=cmap_light, shading='gouraud')\n",
    "for target in wine_dataset.target_names:\n",
    "    index=np.where(wine_dataset.target_names==target)[0][0]\n",
    "    ax1.scatter(X_train[:,2][y_train==index],X_train[:,3][y_train==index],\n",
    "                cmap=cmap_bold,edgecolor='k', s=20, label=target)\n",
    "ax1.set_xlim(x_min,x_max)\n",
    "ax1.set_ylim(y_min,y_max)\n",
    "ax1.legend()\n",
    "ax1.set_xlabel(\"total phenols\")\n",
    "ax1.set_ylabel(\"flavenoids\")\n",
    "ax1.set_title(\"3-Class classification (k = %i, weights = '%s')\"\n",
    "              % (n_neighbors, 'uniform'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c034e46e-1621-4652-8a5a-658abebd27ba",
   "metadata": {},
   "source": [
    "#### Have a look at your \"model\" and think about what this means for making predictions in the test set.\n",
    "\n",
    "### Questions\n",
    ">#### 6. What combinations of total phenols and flavenoids are likely to be difficult to predict in the test set based on the figure?\n",
    ">#### 7. It is impossible to say for sure without extra analyses, but from the figure, do you think we've overfit or underfit the model to the data?\n",
    "\n",
    "#### The next step is to predict the cultvar for a new, unknown wine. Imagine you found a new wine and took measurements of 4 for total phenols and 3.5 for flavenoids. What cultivar does our model propose this wine belongs to? Remember, you don't actually know for sure in this case, though it's pretty clear cut here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e766b5-1a78-47d1-9c24-6bbbbfb13df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a data entry for our new wine\n",
    "new_data=np.array([[13,2.34,4,3.5]])\n",
    "\n",
    "# predict which cultivar this wine is\n",
    "prediction = knn.predict(new_data[:,2:5])\n",
    "\n",
    "# print the prediction\n",
    "print(\"Prediction: {}\".format(prediction))\n",
    "print(\"Predicted target name: {}\".format(wine_dataset['target_names'][prediction]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88304cb1-fe5d-4b2e-9cfa-b4f74f1d1bdf",
   "metadata": {},
   "source": [
    "#### Now is the moment of truth. Does our model predict the cultivar well on an independent dataset (our test set)?\n",
    "\n",
    "#### To ask this question, we throw the X values (total phenols and flavenoids) of the test set at our model and return the predictions. After this, you can print the true values of the cultivar (0 = class_0, 1 = class_1, 2 = class_2) and the predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4be191a-5b65-42a0-a278-f1efceae2370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the cultivars of our test dataset\n",
    "y_pred = knn.predict(X_test[:,2:5])\n",
    "\n",
    "# print the predictions and then the true values\n",
    "print(\"\\nTest set predictions:\\n {}\".format(y_pred))\n",
    "print(\"\\nTest set true values:\\n {}\".format(y_test))\n",
    "\n",
    "# print whether or not the prediction matched the true value\n",
    "print(\"\\nPredictions matched true values:\\n {}\".format(y_pred == y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41392610-4a11-44cc-8356-14acd4a13d5a",
   "metadata": {},
   "source": [
    "### Questions\n",
    ">#### 8. How many wines were asigned to the wrong cultivar?\n",
    "\n",
    "#### We can also ask how accurate the model is - in this case simply as a measure of the proportion of corrections that were correct of the total number of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca10f54-1811-4c21-a238-540e4ee7d434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the proportion of predictions that were correct\n",
    "print(\"Test set score: {:.2f}\".format(knn.score(X_test[:,2:5], y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfb8776-2a9f-4689-8860-70e87a4cdc8d",
   "metadata": {},
   "source": [
    "#### We can plot the values for the test set on the same axes that we plotted the training points.\n",
    "\n",
    "#### As before, ignore the warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9d1cad-c54b-4191-8788-24a15481a1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the model again with the values from the test set as points\n",
    "x_min,x_max = X_train[:,2].min() - 1, X_train[:,2].max()+ 1\n",
    "y_min,y_max = X_train[:,3].min() - 1, X_train[:,3].max()+ 1\n",
    "h=0.02\n",
    "xx,yy = np.meshgrid(np.arange(x_min,x_max,h),np.arange(y_min,y_max,h))\n",
    "Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "cmap_bold = ListedColormap(['darkorange', 'c', 'darkblue'])\n",
    "cmap_light=ListedColormap(['orange', 'cyan', 'cornflowerblue'])\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.pcolormesh(xx, yy, Z, cmap=cmap_light, shading='gouraud')\n",
    "for target in wine.target_names:\n",
    "    index=np.where(wine.target_names==target)[0][0]\n",
    "    ax1.scatter(X_test[:,2][y_test==index],X_test[:,3][y_test==index],\n",
    "                cmap=cmap_bold,edgecolor='k', s=20, label=target)\n",
    "ax1.set_xlim(x_min,x_max)\n",
    "ax1.set_ylim(y_min,y_max)\n",
    "ax1.legend()\n",
    "ax1.set_xlabel(\"total phenols\")\n",
    "ax1.set_ylabel(\"flavenoids\")\n",
    "ax1.set_title(\"3-Class classification (k = %i, weights = '%s')\"\n",
    "              % (n_neighbors, 'uniform'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bdc688-7da6-43cf-8724-95d0ef630097",
   "metadata": {},
   "source": [
    "#### Ask yourself how the model performs. Were you right about the range in which prediction would be more difficult?\n",
    "\n",
    "### Questions\n",
    ">#### 9. Can you think of any downsides of the K nearest neighbours algorithm for this dataset?\n",
    ">#### 10. In a hypothetical scenario, you are given a load more data and your accuracy reduces to .5. Upon investigation you find that your model is overfit. This means that the model is inferring patterns that are true only of the training set, meaining they are not generalisable. How might you adapt your strategy to reduce overfitting but still using K nearest neighbours?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d405a0d0-427c-4026-9c48-4b9f1dcaee43",
   "metadata": {},
   "source": [
    "### Random forests\n",
    "\n",
    "#### Assuming no dreadful miscalculation of timings on the part of your instructor, you will have heard about [Random Forests](https://towardsdatascience.com/random-forest-3a55c3aca46d) in the lecture part of this taster session. I hope you were paying attention.\n",
    "\n",
    "#### Just in case you weren't, Random Forest is a machine learning algorithm that asks questions of our data using decision trees. The model is built by slowly splitting the dataset into purer classes (i.e. moving towards all cultivar 0 or cultivar 2 etc.) according to the X values. A tree might split the data into those with total phenols over 3 and those with flavenoids under 3. Those cultivars with malic acid over 2 might then be split into those with alcohol over 90. This happens sequentially until the nodes of the tree are at some desired purity level (in this case complete purity) or the tree reaches a maximum depth (in this case 8). The random forest generates a given number of trees to make its predictions, here 100.\n",
    "\n",
    "#### Unlike K nearest neighbours above, we will use all 4 features to create our model here. In principle, Random Forests should rely on the most informative features without us specifying them so there isn't much need to worry about which features will be most informative, though too many completely useless features will reduce the efficacy of the model.\n",
    "\n",
    "#### As we've seen, in sklearn, you set up the model first then fit it to your data, which is done below. Have a look at the parameters and think about how the analysis might change under different hyperparameters. To read what each of these parameters does, you can look at the [sklearn Random Forest classifier documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205c7095-befb-4043-af72-1179497b9e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the function we will use\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# set up the model\n",
    "model = RandomForestClassifier(n_estimators = 100,\n",
    "                                       max_depth = 8,\n",
    "                                       max_features='sqrt',\n",
    "                                       min_samples_split=2,\n",
    "                                       n_jobs=1,\n",
    "                                       random_state=0)\n",
    "\n",
    "# fit the model to our training set\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95c8f0b-3b8e-4407-9001-ec2dcbc2bc9e",
   "metadata": {},
   "source": [
    "#### To give you an idea of what the model actually looks like, here is some code that plots the first of the trees in your forest, which should all look identical because you've been given the same starting seed. In each box you are given:\n",
    ">#### a) The decision being made (how the dataset is being split). This is missing in terminal nodes.\n",
    ">#### b) The [GINI importance](https://sam-black.medium.com/calculating-a-features-importance-with-xgboost-and-gini-impurity-3beb4e003b80) of that node. Don't worry too much about GINI unless you are especially interested. For now, you can think of it as the efficacy with which that node splits the tree into purer samples\n",
    ">#### c) The number of samples (wines) that have taken the path leading to that node\n",
    ">#### d) The value - number of samples of each cultivar that have taken the path leading to that node, formatted as [class_0, class_1, class_2]\n",
    ">#### e) the class that would be predicted if a new wine took that path through the tree.\n",
    "\n",
    "#### NOTE: when a wine satisfies the condition set out in the decision in a box it always passes to the left-hand box below and wines that do not satisfy the condition follow the branch on the right-hand side!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a697414-e86c-44a2-b187-883665c44142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the function we will use\n",
    "from sklearn import tree\n",
    "\n",
    "# set up the variables and figure attributes\n",
    "fn=wine['feature_names']\n",
    "cn=wine['target_names']\n",
    "fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (4,4), dpi=800)\n",
    "\n",
    "# create the tree plot\n",
    "tree.plot_tree(model.estimators_[0],\n",
    "               feature_names = fn, \n",
    "               class_names=cn,\n",
    "               filled = True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa867c3-d8ce-4320-8053-993e83f51c89",
   "metadata": {},
   "source": [
    "### Questions\n",
    ">#### 11. How many wines in the training set have flavanoids >= 2.31?\n",
    ">#### 12. Which cultivar would a wine with flavenoids 2,  alcohol 12, malic acid 2 and total phenols 2.5 be predicted to be by the tree displayed here?\n",
    ">#### 13. What is the route taken through this tree by the majority of wines?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7a8189-b1d9-443d-b3b0-2ad5e193a86d",
   "metadata": {},
   "source": [
    "#### Now we predict the cultivar for the wines in our test set and compare to the true values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd36926f-12f9-48b4-9a9e-f368a3d73ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the cultivar of the wine's in our test set\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "# print the test set predictions and true values\n",
    "print(\"Test set predictions:\\n {}\".format(y_pred_test))\n",
    "print(\"\\nTest set true values:\\n {}\".format(y_test))\n",
    "\n",
    "# print the proportion of test set predictions that were correct\n",
    "print(\"\\nTest set score: {:.2f}\".format(model.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcb4f04-5661-49b8-8468-e1bd797a993f",
   "metadata": {},
   "source": [
    "#### So, our random forest has performed much better than our K nearest neighbours analysis. It has predicted the cultivar of far more of the wines in the test set correctly.\n",
    "\n",
    "#### There is an added benefit of the Random Forest algorithm over K nearest neighbours. Remember when I said you don't need to worry too much about GINI? Well the commands below will print the values of GINI for each of the 4 different features. Again, we don't need to worry too much about the specifics of GINI, but you can know that it gives you an idea of the relative importance of each of the features in building the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5991dc9-df4b-4f62-b5dd-2c9d0a2f3098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the gini values for each of the features in our dataset\n",
    "for i in range(4):\n",
    "    print(wine['feature_names'][i])\n",
    "    print(model.feature_importances_[i].round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe4fda2-31fc-47ff-8bca-56ead38008ba",
   "metadata": {},
   "source": [
    "### Questions\n",
    ">#### 14. Order the variables in most important to least important. Compare with the scatter matrix. Does this fit your expectations?\n",
    "\n",
    "#### That's the base practical done. Well done. If you are finished but your appetite for machine learning has not yet been satiated, you can play about with the code to your heart's content. Here are some suggestions.\n",
    ">#### Change the parameters of the K nearest neigbours algorithm and rerun, comparing results with the basic ones.\n",
    ">#### Change the parameters of the Random Forest part of the practical and see how results change. You might find it intersting to see how results change with only 1 tree of max depth 2 or something. This wouldn't be a good analysis but could show how much is acheived by just one feature\n",
    ">#### Implement your own sklearn algorthm on the data - perhaps a neural network. This might be very difficult - I've not done it myself.\n",
    "\n",
    "#### If you need any help with the controls of the jupyter notebook, let the instructor(s) know."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
